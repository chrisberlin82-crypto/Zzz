# ============================================================
# MED Rezeption LIVE - Hetzner CPX Server
# ============================================================
# Verwendung:
#   1. .env Datei erstellen (siehe .env.beispiel)
#   2. docker compose up -d
#   3. docker compose exec ollama ollama pull llama3.1:8b-instruct-q4_K_M
#   4. http://DEINE-SERVER-IP aufrufen
# ============================================================

services:
  # --- Frontend + REST API (Flask) ---
  web:
    build:
      context: ../..
      dockerfile: deploy/hetzner/Dockerfile.live
    restart: unless-stopped
    ports:
      - "127.0.0.1:5000:5000"
    volumes:
      - app_daten:/app/daten
    env_file:
      - .env
    environment:
      - FLASK_ENV=production
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:5000/api/llm/status')"]
      interval: 30s
      timeout: 10s
      retries: 3

  # --- Voicebot Engine (FastAPI + WebSocket + STT + TTS) ---
  voicebot:
    build:
      context: ../..
      dockerfile: deploy/hetzner/Dockerfile.voicebot
    restart: unless-stopped
    ports:
      - "127.0.0.1:8000:8000"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - voicebot_daten:/app/data
      - voicebot_models:/app/models
    env_file:
      - .env
    environment:
      # Ollama laeuft nativ auf dem Host (nicht im Container)
      - MR_LLM_BASE_URL=http://host.docker.internal:11434
      - MR_API_PORT=8000
      - MR_AUDIO_HINTERGRUND_DIR=/app/audio/hintergrund

  # --- Nginx Reverse Proxy ---
  nginx:
    image: nginx:alpine
    restart: unless-stopped
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/conf.d/default.conf:ro
    depends_on:
      - web
      - voicebot

volumes:
  app_daten:
    # WICHTIG: Niemals "docker compose down -v" ausfuehren!
    # Das loescht alle Patientendaten und die Datenbank.
    external: false
  voicebot_daten:
    external: false
  voicebot_models:
    external: false
